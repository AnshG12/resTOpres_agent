\documentclass{article} % For LaTeX2e
\usepackage{arxiv_version,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{ifthen}

\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{natbib} \usepackage{float}
\usepackage{todonotes}
% Vilém: for prompt formatting
\usepackage{courier}
\usepackage{paralist}
\usepackage [autostyle, english = american]{csquotes}
% \MakeOuterQuote{"}
\title{Fluid %Reasoning 
Representations In Reasoning Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\iclrfinalcopy

% \author{Dmitrii Kharlapenko \& Alessandro Stolfo \& Mrinmaya Sachan  \\
% ETH Zurich \& Univesity of Toronto \\
% }

\author{Dmitrii Kharlapenko, Alessandro Stolfo, Mrinmaya Sachan \\
ETH Zurich \\
\AND
Arthur Conmy \\
\And
Zhijing Jin \\
University of Toronto \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\note}[4][]{\todo[author=#2,color=#3,size=\footnotesize,fancyline,caption={},#1]{#4}} 

\newcommand{\ac}[2][]{\textcolor{red}{AC: {#2}}}

\newcommand{\citeneeded}{\textsuperscript{\textcolor{red}{[citation needed]}}}
\newcommand{\cn}{\citeneeded} 

\newcommand{\ale}[2][]{\textcolor{green}{Ale: {#2}}}



%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

% Vilém's suggestion to make it more to the point:
% 
% Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems.
% However, the internal model mechanisms that allow this superior performance remain poorly understood. 
% We present a mechanistic analysis of how QwQ-32B -- a model specifically trained to produce extensive reasoning traces -- process abstract structural information.
% On Mystery Blocksworld -- a semantically obfuscated planning domain we find that QwQ gradually improves its internal understanding of actions and concepts during reasoning.
% The model develops abstract representations that focus on structure rather than specific action names.
% Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss.
% We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub \textit{Fluid Reasoning Representations}.

% Traditional large language models struggle with abstract reasoning tasks. By generating extended chains of thought, reasoning models such as OpenAI's o1 and o3 show dramatic accuracy improvements.

% Instruction-tuned large language models struggle with abstract reasoning tasks.
% Reasoning models like OpenAI's o1 and o3 show dramatic improvements by generating extended chains of thought.
% However, the internal mechanisms underlying this superior performance remain poorly understood.
% We present a mechanistic analysis of how QwQ-32B processes abstract structural information during extended reasoning.
% We analyze the model on Mystery BlocksWorld, a semantically obfuscated planning benchmark.
% We find that QwQ gradually refines its internal understanding of actions and concepts through extended rollouts, developing abstract representations that focus on structure rather than specific action names.
% Through steering experiments, we establish causal evidence for these adaptations: injecting refined representations from successful traces enhances accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss.
% We identify in-context refinement of token representations---which we call \textit{Fluid Reasoning Representations}---as a key factor driving reasoning model performance.

\begin{abstract}
% Instruction-tuned large language models struggle with abstract reasoning tasks. By generating extended chains of thought, reasoning models such as OpenAI's o1 and o3 show dramatic accuracy improvements. However, the internal transformer mechanisms underlying this superior performance remain poorly understood. This work presents an early mechanistic analysis of how reasoning models process abstract structural information during extended reasoning. We analyze QwQ-32B on Mystery BlocksWorld -- a semantically obfuscated benchmark that measures planning and reasoning capabilities.
% We find that QwQ gradually improves its internal understanding of actions and concepts through its extended rollouts, developing abstract representations that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces enhances accuracy, while symbolic representations can replace many specific Mystery BlocksWorld-obfuscated encodings with minimal performance loss. We therefore find that one of the factors driving reasoning model performance is in-context refinement of token representations -- which we call Fluid Reasoning Representations. This provides early mechanistic interpretability into reasoning models.

Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems.
However, the internal model mechanisms that allow this superior performance remain poorly understood. 
We present a mechanistic analysis of how QwQ-32B -- a model specifically trained to produce extensive reasoning traces -- process abstract structural information.
On Mystery Blocksworld -- a semantically obfuscated planning domain -- we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning.
The model develops abstract encodings that focus on structure rather than specific action names.
Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss.
We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub \textit{Fluid Reasoning Representations}.

% Our findings reveal that reasoning models' superior performance stems partly from their ability to dynamically construct problem-specific representational spaces during extended reasoning, providing early mechanistic insights into chains of thought.

% [arthur idea for final sentence alternative]

\end{abstract}

\section{Introduction}

A fundamental question in understanding reasoning language models is whether these models merely pattern-match against memorized associations, or whether they can dynamically construct new representations during problem-solving. This distinction is critical: true reasoning requires adapting internal representations to fit novel problem structures, not just retrieving precomputed solutions. Several reasoning benchmarks have been introduced to assess whether language models exhibit genuine reasoning capabilities or rely primarily on memorized patterns. For instance, ARC-AGI series of benchmarks \citep{chollet2025arcagi2newchallengefrontier,chollet2019measureintelligence} test fluid intelligence through novel visual reasoning tasks designed to minimize reliance on prior knowledge and require genuine abstraction and problem-solving capabilities.

PlanBench \citep{valmeekam2023planbenchextensiblebenchmarkevaluating} takes a complementary approach by creating semantically obfuscated versions of classical planning problems. A particular example is BlocksWorld, a simple planning domain where models must rearrange blocks by applying actions like ``pick up'' and ``put down'' according to explicit rules. These action names appear frequently in pretraining corpora with semantics closely aligned to BlocksWorld, enabling even moderately-sized models to solve standard problems with 60-70\% accuracy by leveraging preexisting associations. It's obfuscated counterpart -- Mystery BlocksWorld breaks these associations while preserving logical structure: all predicates and actions are replaced with semantically unrelated words (e.g., ``pick up'' becomes attack'', ``on top of'' becomes ``craves''). This isolates structural reasoning from pattern-matching -- success requires inferring obfuscated terms from context and constructing abstract representations of domain dynamics.

\citet{valmeekam2024llmscantplanlrms} demonstrate that this semantic obfuscation creates a striking performance gap between model types. Instruction-tuned large language models drop to near-zero accuracy on Mystery BlocksWorld. In contrast, reasoning models -- a new class of models trained via reinforcement learning to generate extended chains of thought \citep{xu2025largereasoningmodelssurvey} -- maintain 20-30\% accuracy even under complete semantic obfuscation. These reasoning models, including OpenAI's o1 \citep{openai2024openaio1card}, DeepSeek R1 \citep{deepseekai2025deepseekr1incentivizingreasoningcapability}, and QwQ-32B \citep{qwq32b}, produce long step-by-step reasoning traces that often span tens of thousands of tokens. Their ability to maintain performance under semantic obfuscation suggests they are doing something fundamentally different: dynamically constructing abstract problem representations during reasoning itself, rather than relying on memorized semantic associations.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/main_2.pdf}
    \caption{\textbf{Overview of our pipeline.} Left: QwQ-32B's accuracy on Standard BlocksWorld is 96\%. Center: Mystery BlocksWorld obfuscates semantics (e.g., ``pick up'' $\to$ ``attack''), reducing QwQ's accuracy to 33\%. During extended reasoning traces, the model progressively refines internal representations of obfuscated actions, developing abstract symbolic encodings (vectors $v_0, \dots, v_3,$ and $u_0, \dots, u_3$ are extracted at different Chain-of-Thought timestamps). Right: Steering experiments inject these refined representations into early reasoning stages, improving accuracy up to 43\%, demonstrating that representational adaptations causally contribute to problem-solving performance.}
    \label{fig:main_diagram}
\end{figure}

% Despite growing interest in understanding these capabilities, mechanistic insights into how extended reasoning traces benefit model performance remain limited. While recent work has identified behavioral patterns in reasoning traces \citep{venhoff2025understandingreasoningthinkinglanguage} and studied simple state tracking in toy reasoning models \citep{zhang2025finitestateautomatainside}, we still lack comprehensive understanding of the internal mechanisms through which long chains of thought improve problem-solving. 
% % \as{This paragraph makes it seems that our work is a marginal contribution on top of this previous paper, while we're doing something substantially different. I'd probably compress this to a single sentence saying that the ICLR paper presents an interesting study of the adaptation of model's representations in a toy setting. We draw inspiration from it to analyze reasoning models in a reasoning/planning setting.}
% Recent work on in-context learning presents an interesting study of how models adapt their internal representations in toy settings when words acquire new meanings within specific contexts \citep{park2025iclrincontextlearningrepresentations}. Drawing inspiration from these insights, we investigate whether similar representational adaptations occur naturally during the extended reasoning processes of reasoning models in reasoning and planning tasks, and whether these adaptations causally contribute to problem-solving performance.

Despite the growing interest in understanding these capabilities, mechanistic insights into how extended reasoning traces benefit model performance remain limited. A major section of reasoning interpretability research focuses on identifying universal reasoning circuits through common token or representation-level components \citep{venhoff2025understandingreasoningthinkinglanguage, bogdan2025thoughtanchorsllmreasoning, lee2025geometryselfverificationtaskspecificreasoning,galichin2025icoveredbaseshere}. However, another possible approach is to examine the problem representations that these circuits operate on. An example of this approach is a recent work on state tracking in toy reasoning models \citep{zhang2025finitestateautomatainside}

% Prior work on in-context learning shows how models adapt internal representations when words acquire new meanings within specific contexts \citep{park2025iclrincontextlearningrepresentations}. Drawing inspiration from these insights, we investigate whether similar representational adaptations occur during extended reasoning in planning tasks, and whether these adaptations causally contribute to problem-solving performance.

We take this representational approach to investigate how reasoning models develop ``understanding'' of the abstract problem structure during their reasoning. Prior work shows that models adapt internal representations when words acquire new meanings in toy in-context learning setups \citep{park2025iclrincontextlearningrepresentations}. We extend their methodology to reasoning traces in planning tasks and establish causal relevance through extensive steering experiments that test whether learned representations actually contribute to problem-solving performance.

% Prior work shows that models adapt internal representations when words acquire new meanings in toy in-context learning setups \citep{park2025iclrincontextlearningrepresentations}. We extend their methodology to reasoning and establish causal relevance: we investigate whether similar adaptations emerge during reasoning in planning tasks, and conduct extensive steering experiments to test whether these representations causally contribute to problem-solving performance. 

% We investigate whether reasoning models develop structured representations during extended reasoning, and whether these representations causally drive performance improvements. Prior work shows that models adapt internal representations when words acquire new meanings in toy in-context learning setups \citep{park2025iclrincontextlearningrepresentations}. We extend their methodology to reasoning traces in planning tasks and establish causal relevance through extensive steering experiments that test whether learned representations actually contribute to problem-solving performance.

We focus our analysis on QwQ-32B \citep{qwq32b}, the most capable open-source reasoning model available, and examine its internal representations while solving Mystery BlocksWorld \citep{valmeekam2023planbenchextensiblebenchmarkevaluating} puzzles. Our central hypothesis is that reasoning models progressively refine their internal representations of problem entities during reasoning, developing context-specific semantics that enable abstract structural reasoning independent of surface-level semantics.

\paragraph{Key Observations.} Our main findings about the internal mechanisms of reasoning models are:

\begin{compactenum} 
\item \textbf{Representational Dynamics} (\Cref{sec:rep}): We observe that QwQ-32B progressively adapts internal representations of actions and predicates during reasoning, with these adaptations converging toward consistent encodings regardless of initial action names.

\item \textbf{Causal Validation} (\Cref{sec:steering}): Through steering experiments, we observe that these representational adaptations causally improve problem-solving performance. Injecting refined representations from successful reasoning traces into early stages of reasoning enhances accuracy on held-out puzzles, with averaged cross-naming representations achieving the strongest effects.

\item \textbf{Symbolic Abstraction} (\Cref{sec:rep_avg,sec:steering_patching}): We observe that adapted representations achieve symbolic abstraction, enabling cross-naming transfer. Models can operate effectively when naming-specific representations are replaced with averaged symbolic representations, suggesting convergence toward abstract structural encodings.
\end{compactenum}

Our findings suggest that the superior performance of reasoning models on abstract reasoning tasks stems, at least partially, from their ability to dynamically construct problem-specific representational spaces during reasoning. This capability represents a fundamental advance in how language models process and represent abstract structural information, with implications for understanding and improving reasoning capabilities in future system.


\section{Background}
\paragraph{BlocksWorld.}
BlocksWorld is a classic planning domain from the International Planning Competitions \citep{ipc1998}. Each puzzle specifies initial and goal block arrangements, with constraints that agents can hold only one block at a time and cannot pick up blocks with others stacked above them. The domain defines four core \textbf{actions}: \textit{pick-up}, \textit{put-down}, \textit{stack}, and \textit{unstack}. The state is described using \textbf{predicates} such as \textit{on(x,y)} (block x is on block y) or \textit{on-table(x)} (block x is on the table). Full prompt can be found in \ref{app:prompt_blocksworld}. We use PlanBench \citep{valmeekam2023planbenchextensiblebenchmarkevaluating} for problem generation and verification. Despite conceptual simplicity, base models fail to achieve perfect accuracy on four-block problems, while reasoning models demonstrate substantially superior performance \citep{valmeekam2024llmscantplanlrms}.

\paragraph{Mystery BlocksWorld.}
Mystery BlocksWorld \citep{valmeekam2023planbenchextensiblebenchmarkevaluating} replaces all predicates and actions with semantically unrelated words through alternative \textbf{namings} -- systematic remappings where, for example, the action \textit{pick-up} becomes \textit{attack} and the predicate \textit{on(x, y)} becomes \textit{craves(x, y)} (prompt example in \Cref{app:prompt}). Each naming provides a complete semantic obfuscation that preserves the underlying logical structure while causing dramatic performance degradation. Success requires models to operate on abstract structural relationships and dynamically construct new semantic mappings from these obfuscated terms -- capabilities reasoning models demonstrate significantly better than base LLMs.

We generated \textbf{14 additional naming variants} beyond the original, creating 15 different semantic obfuscations of the same domain structure (see \Cref{app:variants}). We selected this domain because its fixed action space and strict rules provide a clear concept set for both the model and our analysis.

\paragraph{Terminology.} 
\label{sec:terms} We refer to each unique initial-goal state combination as a \textbf{puzzle} and each mapping variant as a \textbf{naming}. Our analysis focuses on 300 four-block puzzles, each mapped across all 15 mystery namings.

\subsection{Initial Evaluations}
\label{sec:evals}

We conducted evaluations of various models on our BlocksWorld puzzle dataset to establish baseline performance and validate our choice of QwQ-32B for detailed analysis. They are available in \Cref{tab:blocksworld_performance}.

 Reasoning models consistently outperform standard LLMs on both regular and Mystery BlocksWorld tasks, though open-source reasoning models of moderate size remain limited, with DeepSeek distillation models showing particularly poor Mystery BlocksWorld performance. QwQ-32B demonstrates exceptional performance on both variants, with successful Mystery BlocksWorld solutions typically requiring 15-20k token reasoning traces -- substantially longer than regular BlocksWorld problems and crucial for the semantic adaptation process investigated in this work. While Nemotron generates similarly long reasoning traces, QwQ-32B achieves superior accuracy across most mystery namings we have generated. We also provide reasoning behavior breakdown similar to \cite{venhoff2025understandingreasoningthinkinglanguage} in \Cref{app:behavior}.


\begin{table}[h!]\small
\centering
\caption{Performance comparison of models on BlocksWorld and \textbf{Naming 1} Mystery BlocksWorld puzzles (all 300). ``Accuracy Preserved'' indicates the percentage of accuracy retained. ``Tokens'' represents the average length of the CoT. $^*$\textit{Llama Nemotron computes the correct answer more often, but often answers with incorrect formatting which the evaluation suite cannot parse.}}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \multicolumn{2}{c}{\textbf{BlocksWorld}} & \multicolumn{2}{c}{\textbf{Mystery}} & \textbf{Accuracy Preserved} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& \texttt{Acc} & \texttt{Tokens} & \texttt{Acc} & \texttt{Tokens} & \\
\midrule
\multicolumn{6}{l}{\textbf{Regular LLMs}} \\
\midrule
GPT-4.1 (CoT) & 0.92 & 556 & 0.18 & 3837 & \textbf{20\%} \\
Qwen2.5-32B & 0.21 & 71 & 0.00 & 1390 & \textbf{0\%} \\
Qwen2.5-32B-Instruct (CoT) & 0.38 & 353 & 0.00 & 1479 & \textbf{0\%} \\
Llama 3.3 70B Instruct (CoT) & 0.40 & 760 & 0.02 & 1142 & \textbf{5\%} \\
\midrule
\multicolumn{6}{l}{\textbf{Reasoning Models}} \\
\midrule
DeepSeek-R1-Distill-Qwen-32B & 0.81 & 2387 & 0.08 & 8500 & \textbf{10\%} \\
DeepSeek-R1-Distill-Llama-70B & 0.66 & 2674 & 0.10 & 10636 & \textbf{15\%} \\
Llama Nemotron Super 49B v1 & 0.48$^*$ & 1162 & 0.19 & 9200 & \textbf{40\%} \\
QwQ-32B & 0.96 & 3633 & 0.35 & 16186 & \textbf{36\%} \\
\bottomrule
\end{tabular}
\label{tab:blocksworld_performance}
\end{table}

\subsection{Mystery Performance Analysis}
\label{sec:mystery_performance}
QwQ-32B's accuracy varies \textit{dramatically} across mystery namings, from 0.05 to 0.47. The model performs worst on namings suggesting reversible operations ("open/close," ``plant/harvest") or coherent alternative domains (legal proceedings, gardening cycles), while abstract philosophical terms, mixed sensory modalities, and semantically incoherent combinations enable better performance. This suggests that \textit{semantically connected actions and predicates make it much harder for the model to abstract away} from their initial meanings. To verify this hypothesis, we generated several additional naming variants beyond the original 15, though these are not included in most experiments (see \Cref{app:mystery_performance} for all results). Our steering experiments on the early layers (\ref{sec:steering_positive} also suggest a connection between the semantics of the replacement words and the final performance.

As a special case, \textit{Mystery naming 3} uses random strings, causing the model to explicitly recognize the task as BlocksWorld and directly map obfuscated terms to actions, as evidenced by substantially shorter reasoning traces (~2k vs 15-20k tokens) and manual trace analysis. This suggests that some version of Mystery BlocksWorld was present in QwQ-32B's training data. Since we do not observe the model recognizing the BlocksWorld domain in reasoning traces for other namings, we believe it genuinely attempts to discover solutions rather than retrieving them from memory in those cases. We exclude naming 3 from representational analyses as it bypasses the semantic adaptation process under investigation.

\subsection{Representation Collection}
\label{sec:representation_collection}

\paragraph{Overview.}
Our goal is to extract vector representations of actions and predicates from the model's internal activations as it reasons about BlocksWorld puzzles. We build on the methodology of \citet{park2025iclrincontextlearningrepresentations}, applying it to reasoning traces collected from BlocksWorld and Mystery BlocksWorld puzzles from our dataset.

\paragraph{Representation extraction.}
Given a batch $\mathcal{B}$ of reasoning traces, action (predicate) $a$, naming $N$, layer $L \in \{1, \ldots, L_{\max}\}$, and timestamp $T$ (token position in the sequence), we extract the representation $\mathbf{r}_a^{N,L,T}$ as follows:

\begin{enumerate}
    \item \textbf{Window extraction:} For each trace in $\mathcal{B}$, consider tokens in the window $[T-w, T)$ where $w$ is the window size (we use $w=100$).

    \item \textbf{Token selection}. Find all token sequences that encode $a$. (For example \text{[``att'', ``ack'']} for ``attack''). For each sequence also include one token before.  
    
    \item \textbf{Averaging:} Extract hidden states $\mathbf{h}_i^L$ at layer $L$ for all matched token positions. Average within each matched sequence, then average across all sequences in the batch:
    \begin{equation}
        \mathbf{r}_a^{N,L,T} = \frac{1}{|\mathcal{S}|} \sum_{s \in \mathcal{S}} \left( \frac{1}{|s|} \sum_{i \in s} \mathbf{h}_i^L \right)
    \end{equation}
    where $\mathcal{S}$ is the set of all matched sequences across the batch, and $s$ denotes the token positions in a matched sequence.
\end{enumerate}

We repeat this process across multiple timestamps (every 200 tokens) and all layers to obtain a comprehensive set of representations for each action (predicate).

\paragraph{In-naming representations.}
For each mystery naming $N$, we collect \textbf{in-naming representations} $\mathbf{r}_a^{N,L,T}$ for each action (predicate) $a$ at all layers $L$ and multiple timestamps $T$. To isolate the action direction within a naming, we create \textit{centered} representations \citet{venhoff2025understandingreasoningthinkinglanguage}:
\begin{equation}
    \tilde{\mathbf{r}}_a^{N,L,T} = \mathbf{r}_a^{N,L,T} - \frac{1}{|\mathcal{A}|} \sum_{a' \in \mathcal{A}} \mathbf{r}_{a'}^{N,L,T}
\end{equation}
where $\mathcal{A}$ is the set of all actions (or predicates) in the domain. 

\paragraph{Cross-naming representations.}
To extract the abstract, symbolic meaning of an action (predicate) independent of its surface form, we compute \textbf{cross-naming representations} by averaging centered in-naming representations across all namings:
\begin{equation}
    \bar{\mathbf{r}}_a^{L,T} = \frac{1}{|\mathcal{N}|} \sum_{N \in \mathcal{N}} \tilde{\mathbf{r}}_a^{N,L,T}
\end{equation}
where $\mathcal{N}$ is the set of all namings. This averaging operation should cancel out naming-specific surface features and preserve only the invariant semantic core ~-- the actual logical meaning of the concept in the Mystery BlocksWorld context. For example, the cross-naming representation for \textit{pick-up} should capture its abstract meaning as "grasping a clear block from the table" regardless of whether it's called ``pick-up'', ``attack'', ``illuminate'', or any other obfuscated term. 

\section{Representational Studies}
\label{sec:rep}
Our main hypothesis is that reasoning models progressively refine their internal representations of problem entities during extended reasoning. We call such refined representations Fluid Reasoning Representations, named after fluid reasoning in humans \citep{fluid, wu2025understandingllmsfluidintelligence}. This process develops context-specific semantics that enable abstract structural reasoning independent of surface-level word meanings. We test this hypothesis by analyzing how QwQ-32B's \citep{qwq32b} representations of actions and predicates evolve while solving Mystery BlocksWorld \citep{valmeekam2023planbenchextensiblebenchmarkevaluating} puzzles.

\subsection{Cross-Naming Representational Convergence}

If our hypothesis is correct, then semantically equivalent actions should converge to similar internal encodings across different mystery namings, regardless of their surface-level differences.

As a first step to investigate our hypothesis, we extract \textbf{in-naming} representations from Mystery naming 1 at timestamps 2k, 4k, 7k, and 10k tokens, then compute cosine similarities between these and centered representations from all timesteps across all other mystery namings, averaging the results.  On \Cref{fig:diff_timestamps}, we plot two lines for each Mystery 1 timestamp: one for the average similarity of an action with corresponding action from other namings and another one for average similarities of the action with different actions from the other namings. The figure shows that except for timestamp 2k cross-naming similarity increases substantially during reasoning, plateauing around 7,000 tokens --- typically coinciding with the model switching from examining single actions to trying building chains out of them. (\Cref{app:behavior}).

We also observe that similarities with different actions are always lower than with the corresponding ones. Relatively high ($	\approx 0.2$) similarity is caused by representations of ``stack" and ``unstack" being closer to each other, than to ``pick up" and ``put down".  

\begin{minipage}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/mystery_1_different_timestamps.pdf}
    \captionof{figure}{Average similarity of representations from other namings with naming 1 representations, extracted from different timestamps.}
    \label{fig:diff_timestamps}
\end{minipage}
\hfill
\begin{minipage}[t]{0.55\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pca_layer_analysis_7k_short.pdf}
    \captionof{figure}{Layer-wise PCA of action representations from different mystery namings extracted at 7k tokens. More layers is \Cref{app:pca}.}
    \label{fig:pca_7k}
\end{minipage}

To visualize how representations cluster across namings, we perform PCA analysis on action representations extracted at 7k tokens from layers 10 and 40. \Cref{fig:pca_7k} demonstrates that semantically equivalent actions cluster together regardless of their surface-level naming, with clustering becoming apparent in deeper layers. 

\subsection{Similarity with Average and Original BlocksWorld}

\label{sec:rep_avg}
To better understand the nature of representational convergence, we examine similarities between naming-specific representations and average representations computed across all namings. This analysis reveals two important patterns that were obscured in the pairwise comparison.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.44\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/similarities_avg.pdf}
        \caption{Similarities with average representations}
        \label{fig:sims_avg}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.52\textwidth}
        \centering
        \includegraphics[width=0.86\textwidth]{figures/similarities_avg_from_clean.pdf}
        \caption{Similarities with clean BlocksWorld representations}
        \label{fig:sims_clean}
    \end{subfigure}
    \caption{\textbf{Similarity with cross-naming representations between Mystery and Original BlocksWorld traces.} (a) Shows average similarities of centered action/predicate representations from all timestamps in Mystery Blocksworld traces with cross-naming representations extracted at 7k tokens. Note that similarities between different actions become increasingly negative. (b) Shows average similarities of clean BlocksWorld representations from all timestamps with cross-naming representations extracted at 7k tokens. Plot for predicates is absent, since it's much harder to identify their tokens in regular BlocksWorld traces.}
    \label{fig:representational_convergence}
\end{figure}

First, when comparing centered representations with their corresponding average representations (\Cref{fig:sims_avg}), similarity increases substantially during reasoning, plateauing around 7,000 tokens. Crucially, similarities between different actions become increasingly negative as reasoning progresses. This shows the model actively differentiates between action types while developing shared encodings for equivalent actions across namings.

Second, we compare mystery naming representations at 7k tokens with clean BlocksWorld representations across all timestamps (\Cref{fig:sims_clean}). Similarity with clean BlocksWorld starts near zero and increases substantially as clean reasoning progresses. This shows the model develops similar symbolic representations even with preserved semantic content, indicating that representational adaptation is a fundamental reasoning mechanism, not just compensation for semantic obfuscation.

\subsection{Base Model Comparison}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/mystery_1_qwq_base.pdf}
        \caption{Cross-naming similarity comparison}
        \label{fig:sims_pair_base_qwq}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/from_clean_combined.pdf}
        \caption{Clean BlocksWorld similarity comparison}
        \label{fig:sims_pair_base_qwq_from_clean}
    \end{subfigure}
    \caption{Average similarity of representations extracted from the 7k timestamp, plotted for both QwQ and its base model on QwQ traces. (a) Shows similarity of representations from other namings with naming 1 representations (averaged across all other namings). (b) Shows similarity of representations from original BlocksWorld traces with representations from different mystery namings (averaged across them).}
    \label{fig:base_model_comparison}
\end{figure}

We tested whether representational adaptation is specific to reasoning models by analyzing both QwQ and its base model processing identical QwQ-generated traces. Both models exhibit similar adaptation dynamics (\Cref{fig:sims_pair_base_qwq}), with the base model adapting slightly more slowly ~- likely due to processing unnatural traces. Both show comparable convergence toward shared symbolic representations (\Cref{fig:sims_pair_base_qwq_from_clean}).

This finding, combined with prior work on in-context learning \citep{park2025iclrincontextlearningrepresentations}, indicates representational adaptation is an inherent property of large language models rather than a specialized reasoning model feature. The difference is that reasoning models naturally produce the extended context needed to use these adaptations.


\section{Causal validation}
\label{sec:steering}

The representational analysis in \Cref{sec:rep} reveals that QwQ-32B dynamically adapts representations of actions and predicates beyond their original lexical meanings, with adaptations appearing independent of original word semantics. 

This suggests two testable hypotheses: \textbf{(1)} representational adaptations reflect genuine improvements in understanding abstract puzzle structure, and \textbf{(2)} adapted representations achieve symbolic abstraction that transcends original tokens, enabling transfer across naming schemes. We design steering experiments to test whether learned representations contain actionable structural knowledge and can function independently of their linguistic context.
\subsection{Positive Steering}
\label{sec:steering_positive}

To test hypothesis \textbf{(1)} -- that representational adaptations reflect genuine improvements in understanding of abstract puzzle structure -- we conduct positive steering experiments. We inject action and predicate representations extracted from late stages of reasoning traces into early reasoning traces and measure accuracy improvements. We find that these representations boost accuracy, demonstrating they encode operationally useful structural understanding. Furthermore, cross-naming representations improve performance across different namings, providing evidence for hypothesis \textbf{(2)}: symbolic abstraction that transcends specific lexical forms.

\paragraph{Experimental Setup.} Our steering procedure selects a steering layer $L$, token window $[t_{\text{start}}, t_{\text{end}})$, and steering scale $s$. We collect three types of steering vectors at layer $L$ from the 40 correctly solved puzzles: \textbf{(1)} centered \textbf{in-naming} representations $\tilde{\mathbf{r}}_a^{N,L,T}$ for all actions and predicates, \textbf{(2)} \textbf{cross-naming} representations $\bar{\mathbf{r}}_a^{L,T}$ averaged across all namings, and \textbf{(3)} random Gaussian vectors $\mathbf{v}_{\text{rand}}[a]$ scaled to match the norm of \textbf{in-naming} representations. We extract prefixes of $t_{\text{end}}$ tokens from a hold-out set of 100 different 4-block problem rollouts as our intervention dataset.

For each prefix, we identify token indices $i$ corresponding to action or predicate $a$, obtain hidden states $\mathbf{h}_i^L$ at layer $L$, and apply the following norm-preserving intervention:
\begin{align}
    \mathbf{h'}_i^L &= s \cdot \mathbf{h}_i^L + (1-s) \cdot \mathbf{v}_{\text{type}}[a], \\
    \mathbf{h}_i^L &\leftarrow \mathbf{h'}_i^L \cdot \frac{\|\mathbf{h}_i^L\|_2}{\|\mathbf{h'}_i^L\|_2},
\end{align}
where $\mathbf{v}_{\text{type}}[a] \in \{\tilde{\mathbf{r}}_a^{N,L,T}, \bar{\mathbf{r}}_a^{L,T}, \mathbf{v}_{\text{rand}}[a]\}$ depending on the experiment condition. This procedure adds the refined representation while preserving activation magnitude. We measure accuracy improvement on steered puzzles compared to non-steered baseline. We selected scale $s=\frac{2}{3}$ after a sweep on layer 20 using \textbf{in-naming} representations (\Cref{app:hyperparams}). The steering window is $[1500, 2500]$.

\begin{minipage}[h!]{0.51\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/steering_improvement_layers.pdf}
    \captionof{figure}{Accuracy improvement after positive steering averaged across mystery namings (excluding Naming 3). \textbf{Takeaways:} (i) Even random early-layer interventions ($L\leq$10) already improve accuracy, suggesting they help remove surface-level naming associations. (ii) From $L\geq 20$ onward, steering with refined representations is most beneficial: \emph{cross-naming} $>$ \emph{in-naming} $\gg$ random. Error bars show s.e. across namings. See Sec. \ref{sec:steering_positive}.}
    \label{fig:positive_steering_layers}
\end{minipage}
\hfill
\begin{minipage}[h!]{0.475\textwidth}
    \centering
    % \vspace{-4mm}
    \includegraphics[width=0.89\textwidth]{figures/patching_scales.pdf}
    \vspace{-2mm}
    \captionof{figure}{Mean accuracy difference between symbolic patching and shuffled control across scaling factors $s$. \textbf{Takeaways:} Matched \emph{symbolic} representations outperform the shuffled baseline. This supports the \emph{symbolic abstraction} hypothesis: the model can operate when naming-specific activations are replaced by naming-agnostic symbolic vectors. Error bars show s.e. across namings. See \Cref{sec:steering_patching}.}
    \label{fig:patching_scales}
\end{minipage}


\paragraph{Results.}
Figure~\ref{fig:positive_steering_layers} shows accuracy improvements after positive steering, averaged across namings (excluding naming 3). Even random noise improves accuracy on early layers, suggesting that in this case disrupting original semantic associations helps overcome interference from misleading word meanings, which is consistent with our initial evaluations (\Cref{sec:mystery_performance}).

Steering with \textbf{in-naming} representations improve performance with a notable drop before layer 20, where PCA analysis reveals the onset of action representation separation (\Cref{app:pca}). This suggests accuracy improvements from layer 20 onward stem from representations becoming genuinely meaningful rather than noise effects. As a side note, we attempted to steer the \textbf{base model} using traces from the reasoning model, but did not achieve noticeable improvement, as the base model could not operate effectively with such long contexts without breaking down.

\textbf{Cross-naming} representations achieve the highest impact across all layers, reinforcing that these adaptations encode abstract problem structure rather than naming-specific artifacts. Together, these results support our hypothesis that learned adaptations contain meaningful structural understanding.

We also note a significant layer dependence for steering efficiency with different namings. This, along with some namings being much less responsive to steering, may be one of the reasons why average accuracy boost is relatively low (compared to gains of up to \textbf{10\%} in some cases (\Cref{app:mystery_performance})). 

\subsection{Symbolic Patching}
\label{sec:steering_patching}

To test hypothesis \textbf{(2)} -- that adapted representations achieve symbolic abstraction independent of surface forms -- we conduct a patching experiment that replaces naming-specific representations with cross-naming ``symbolic'' representations and tests whether the model can operate effectively without access to the original naming-specific encodings.

\paragraph{Symbolic Representation Construction.} We construct symbolic representations to be minimally out-of-distribution while capturing abstract structural information. We collect centered \textbf{cross-naming} representations $\bar{\mathbf{r}}_a^{L,T}$ for each action and predicate, compute the overall mean $\bar{\mathbf{r}}_{\text{mean}}^{L,T}$ across all actions (and separately for predicates), and construct symbolic representations as:
\begin{equation}
    \mathbf{r}_{\text{symbolic}}[a] = \bar{\mathbf{r}}_{\text{mean}}^{L,T} + s \cdot \bar{\mathbf{r}}_{a}^{L,T}
\end{equation}
where $s$ is a mixing scale and $\bar{\mathbf{r}}_{a}^{L,T}$ is the centered cross-naming representation of action $a$.

\paragraph{Experimental Design.} Since the model maintained reasonable accuracy even when all actions were replaced with a single vector, we use a comparative approach: \textbf{(1)} Symbolic Patching replaces residual stream activations for action/predicate tokens with corresponding symbolic representations, and \textbf{(2)} Shuffled Patching uses randomly permuted symbolic representations as control.

We patch token window [2000, 4000] on all layers until the selected end layer, then measure accuracy difference $\text{Acc}_{\text{symbolic}} - \text{Acc}_{\text{shuffled}}$. Figure~\ref{fig:patching_scales} confirms that properly matched symbolic representations consistently outperform shuffled ones across scaling factors, supporting meaningful symbolic abstraction.


\subsection{Negative Steering} \label{sec:steering_negative}

To further test hypothesis \textbf{(1)}, we conduct negative steering by subtracting converged in-naming representations from model activations, aiming to disable the representational adaptations. Since steering interventions can easily degrade performance, we use shuffled representations as a comparative baseline. Performance degradation beyond the shuffled control demonstrates the adaptations encode operationally important structural understanding rather than arbitrary activation patterns.

\paragraph{Experimental Design.} We perform interventions across token window [2000, 4000] on multiple layers, subtracting centered naming representations extracted from the 4k timestamp (selected as these are near convergence while at our window's end, see \Cref{fig:diff_timestamps}). We use shuffled representations as control, as random vectors provided insufficient baseline strength.

With optimal layer selection, negative steering shows 2.9\% accuracy mean difference with control (full results in \Cref{app:negative_steering}). This reinforces that representational adaptations play a crucial role in problem-solving, as disrupting learned representations leads to measurably worse performance even when controlling for general intervention effects..

\section{Related Work}

% https://arxiv.org/abs/2411.02344
% https://www.goodfire.ai/research/under-the-hood-of-a-reasoning-model
% https://arxiv.org/pdf/2402.18312
% https://arxiv.org/abs/2406.02128
% https://arxiv.org/abs/2503.01307
% https://aclanthology.org/2025.acl-long.339.pdf
% https://arxiv.org/pdf/2507.12638
% https://arxiv.org/abs/2504.07912
% https://arxiv.org/abs/2504.13837
% https://arxiv.org/pdf/2310.14491

% \paragraph{RL for Reasoning}
% These guys do different RLs for reasoning and they work: \todo{Add papers}.

% \citep{tang2025unlockinggenerallongchainofthought} is an example of a representations-level analysis that enables better reasoning in LLMs.

% \paragraph{Representations and Steering.}
% A growing line of work focuses on identifying meaningful directions in model representation spaces and using them to modify model behavior. These directions may encode abstract concepts like refusal \citep{arditi2024refusallanguagemodelsmediated}, emotions and truthfulness \citep{zou2025representationengineeringtopdownapproach}. Beyond concept-based representations, single directions may also contain complex functional and structural information, such as generalized task definitions from in-context learning examples \citep{todd2024functionvectorslargelanguage,hendel2023incontextlearningcreatestask}, new meanings of words in in-context learning \citep{park2025iclrincontextlearningrepresentations}, or even reasoning behavior itself \citep{zhao2025activationcontrolefficientlyeliciting}.

\paragraph{Interpretability of Language Models' Representations.}
Recent mechanistic interpretability research has converged on identifying meaningful directions in model representation spaces. Studies have demonstrated that large language models encode diverse features as linear directions in their activation spaces, including truthfulness \citep{li2023inferencetime, azaria-mitchell-2023-internal, marks2024the, zou2025representationengineeringtopdownapproach}, sentiment \citep{tigges2024language}, sycophancy \citep{perez-etal-2023-discovering, rimsky-etal-2024-steering, sharma2024towards}, factual knowledge \citep{gurnee2024language}, and refusal behavior \citep{arditi2024refusal}. Complementing these supervised approaches, sparse autoencoders have emerged as powerful tools for discovering feature directions in an unsupervised manner, revealing interpretable features at scale \citep{bricken2023monosemanticity,huben2024sparse,templeton2024scaling}. These findings support the linear representation hypothesis; i.e., that neural networks encode semantic concepts as linear directions in high-dimensional activation spaces \citep{NIPS2013_9aa42b31, NIPS2016_a486cd07, elhage2021mathematical, nanda-etal-2023-emergent, pmlr-v235-park24c, olah2024lrh}. Beyond concept-based representations, single directions may also contain complex functional and structural information, such as generalized task definitions from in-context learning examples \citep{todd2024functionvectorslargelanguage,hendel2023incontextlearningcreatestask}, new meanings of words in in-context learning \citep{park2025iclrincontextlearningrepresentations}, user-specified instructions \citep{stolfo2025improving}, or even reasoning behavior itself \citep{zhao2025activationcontrolefficientlyeliciting}.


\paragraph{Reasoning Interpretability.} 
% Previous work 
A major part of reasoning interpretability research focuses on identifying universal reasoning circuits through common reasoning components. These can be key intermediate sentences or ``thought anchors'' \citep{bogdan2025thoughtanchorsllmreasoning}, reasoning behaviors like uncertainty expression and backtracking \citep{gandhi2025cognitivebehaviorsenableselfimproving,venhoff2025understandingreasoningthinkinglanguage}, self-verification directions \citep{lee2025geometryselfverificationtaskspecificreasoning}, reasoning-related sparse autoencoder features \citep{galichin2025icoveredbaseshere}. As an alternative representations-based approach, \cite{zhang2025finitestateautomatainside,hou2023mechanisticinterpretationmultistepreasoning} study state tracking or contents of a reasoning tree in toy transformers, while \cite{arefin2025seqvcrpreventingcollapseintermediate} looks at representations during reasoning from a compression perspective. \cite{dutta2024thinkstepbystepmechanisticunderstanding} investigates attention patterns and shifts of representations spaces during different reasoning behaviors representing a mix of both approaches. Finally, \cite{ward2025reasoningfinetuningrepurposeslatentrepresentations} compares representations in reasoning and base models, finding that reasoning-finetuning repurposes directions already present in base model activations.

% \paragraph{Model Diffing.}
% Another rapidly developing approach to studying complex model behaviors involves analyzing differences between models trained on different data. For example, comparing representations in reasoning and base models \citep{ward2025reasoningfinetuningrepurposeslatentrepresentations}, identifying new concepts in chat-tuned models \citep{minder2025overcomingsparsityartifactscrosscoders}, or studying context-sensitivity capabilities \citep{minder2025controllablecontextsensitivityknob}.


\section{Limitations}

We focus on a single reasoning model (QwQ-32B) and a single domain (BlocksWorld). While this does not cover the full diversity of reasoning tasks, BlocksWorld offers a particularly clean testbed: it has a small, well-defined set of actions and predicates, clear structural rules, and easily controlled obfuscations. This makes it possible to isolate representational adaptation in a way that would be difficult in domains with unconstrained concept spaces, such as open-ended mathematics or natural language reasoning. We expect these findings to generalize to other structured planning setups with fixed action spaces (e.g., Towers of Hanoi), though verifying this and testing whether the patterns extend to less constrained domains remains future work.

Our steering and patching intervention methods are deliberately simple, chosen to remain tractable on reasoning traces that often span 15–20k tokens. More targeted or fine-grained causal tools could sharpen the picture, but even our coarse interventions reveal measurable effects. Similarly, computational limits prevented extensive hyperparameter sweeps or decoding strategy comparisons, yet the observed representational trends were consistent across multiple obfuscations. We also note that shuffled-control experiments (\Cref{app:shuffled_steering}) reveal unexpected gains around later layers (notably layer~30), suggesting that some aspects of late-layer representational dynamics remain to be explained in future work.

\section{Conclusion}

This work analyzed how a reasoning-oriented language model (QwQ-32B) processes abstract structural information during extended reasoning. We presented three main observations. First, the model progressively refines internal representations of actions and predicates over long reasoning traces, converging toward abstract encodings that are less dependent on surface-level semantics. Second, steering experiments suggest that these representational adaptations are not merely descriptive but can causally influence problem-solving performance: injecting refined representations tends to increase accuracy, while disrupting them tends to decrease it. Third, we observed evidence of symbolic abstraction, where representations transfer across different obfuscated namings, suggesting a degree of naming-invariant structural encoding.

Taken together, these results suggest that the superior performance of reasoning models on abstract reasoning tasks may stem in part from their ability to dynamically construct context-specific representational spaces during extended reasoning. While preliminary, our findings highlight representational refinement as a promising direction for understanding the internal mechanisms of reasoning models and contribute to a growing body of work on the interpretability of long-form reasoning traces.

\section*{Acknowledgements}

We thank Open Philanthropy for their financial support of this research. We are grateful to Nebius for providing the computational resources that made this work possible. We also thank Matthew Wearden, Roderick Wu, and Vilém Zouhar for valuable discussions and feedback on earlier versions of this manuscript. Alessandro Stolfo acknowledges the support of armasuisse Science and Technology through a CYD Doctoral Fellowship


% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}
\newpage 

\appendix

\section{BlocksWorld prompt example}
\label{app:prompt_blocksworld}

% % Vilém: the prompt went outside of the page
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\begin{lstlisting}
I am playing with a set of blocks where I need to arrange the blocks into stacks. Here are the actions I can do

Pick up a block
Unstack a block from on top of another block
Put down a block
Stack a block on top of another block

I have the following restrictions on my actions:
I can only pick up or unstack one block at a time.
I can only pick up or unstack a block if my hand is empty.
I can only pick up a block if the block is on the table and the block is clear. 
A block is clear if the block has no other blocks on top of it and if the block is not picked up.
I can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block.
I can only unstack a block from on top of another block if the block I am unstacking is clear.
Once I pick up or unstack a block, I am holding the block.
I can only put down a block that I am holding.
I can only stack a block on top of another block if I am holding the block being stacked.
I can only stack a block on top of another block if the block onto which I am stacking the block is clear.
Once I put down or stack a block, my hand becomes empty.
Once you stack a block on top of a second block, the second block is no longer clear.

Here is an example problem:

[STATEMENT]
As initial conditions I have that, Block B is clear, Block C is clear, the hand is empty, Block C is on top of Block A, Block A is on the table, Block B is on the table.
My goal is to have that Block A is on top of Block C and Block B is on top of Block A

My plan is as follows:

[PLAN]
unstack Block C from on top of Block A
put down Block C
pick up Block A
stack Block A on top of Block C
pick up Block B
stack Block B on top of Block A
[PLAN END]

\end{lstlisting}

\section{Mystery prompt example}
\label{app:prompt}

% % Vilém: the prompt went outside of the page
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\begin{lstlisting}
I am playing with a set of objects. Here are the actions I can do:
   Attack object
   Feast object from another object
   Succumb object
   Overcome object from another object

I have the following restrictions on my actions:
    To perform Attack action, the following facts need to be true: Province object, Planet object, Harmony.
    Once Attack action is performed the following facts will be true: Pain object.
    Once Attack action is performed the following facts will be false: Province object, Planet object, Harmony.
    To perform Succumb action, the following facts need to be true: Pain object.
    Once Succumb action is performed the following facts will be true: Province object, Planet object, Harmony.
    Once Succumb action is performed the following facts will be false: Pain object.
    To perform Overcome action, the following needs to be true: Province other object, Pain object.
    Once Overcome action is performed the following will be true: Harmony, Province object, Object Craves other object.
    Once Overcome action is performed the following will be false: Province other object, Pain object.
    To perform Feast action, the following needs to be true: Object Craves other object, Province object, Harmony.
    Once Feast action is performed the following will be true: Pain object, Province other object.
    Once Feast action is performed the following will be false:, Object Craves other object, Province object, Harmony.

Here is an example problem:
[STATEMENT]
As initial conditions I have that, province Block B, province Block C, harmony, Block C craves Block A, planet Block A, planet Block B.
My goal is to have that Block A craves Block C and Block B craves Block A.
My plan is as follows:
[PLAN]
feast Block C from Block A
succumb Block C
attack Block A
overcome Block A from Block C
attack Block B
overcome Block B from Block A
[PLAN END]
\end{lstlisting}


\section{Behavior analysis}
\label{app:behavior}
Through manual investigation of DeepSeek and QwQ reasoning traces, we identified recurring behavioral patterns in Mystery BlocksWorld solving. Models begin with \textbf{comparative analysis}, examining initial and goal states to identify conflicting predicates. They then alternate between \textbf{recursive search} (working backwards from goals to identify required actions) and \textbf{exploration} (experimenting with actions to discover achievable states). These exploratory behaviors occupy the first half of reasoning traces. The second phase involves \textbf{plan formulation}, where models construct action sequences and verify validity, iteratively rebuilding when conflicts arise. The final phase consists of \textbf{plan verification}, where models validate solutions before committing to answers.

\newpage

\section{Layer-wise PCA}
\label{app:pca}
\cref{fig:pca_7k_all}  contains layer-wise PCA for action representations of different reasoning models. The clustering patterns become more pronounced in deeper layers, with clear separation between action types emerging around layers 20-30. 

\begin{figure}[h!]
\centering 
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pca_layer_analysis_7k.pdf}
    \caption{QwQ}
    \label{fig:pca_7k_qwq}
\end{subfigure}

\vspace{1em}

\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pca_layer_analysis_7k_ds_ds_traces.pdf}
    \caption{Qwen-32B DeepSeek}
    
\end{subfigure}
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pca_layer_analysis_7k_nemo.pdf}
    \caption{Llama Nemotron 49B}
\end{subfigure}
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pca_layer_analysis_7k_seed_instruct.pdf}
    \caption{Seed-OSS-36B-Instruct}
\end{subfigure}

\caption{Layer-wise PCA of action representations from different mystery namings extracted at 7k tokens for (a) QwQ, (b) Qwen-32B DeepSeek, (c) Llama Nemotron 49B and (d) Seed-OSS-36B-Instruct}
\label{fig:pca_7k_all}
\end{figure}

\section{Cross-Model Similarity Analysis}
\label{app:cross_model_similarities}

To validate that representational convergence is not unique to QwQ-32B, we analyzed action and predicate representations across multiple reasoning models. \Cref{fig:sims_avg_all_models} shows how centered action and predicate representations from different timestamps converge toward cross-naming average representations extracted at 7k tokens, analogous to the analysis in \Cref{sec:rep_avg}.

\begin{figure}[h!]
\centering 
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/similarities_avg_ds_ds_traces.pdf}
    \caption{Qwen-32B DeepSeek}
    \label{fig:sims_avg_ds}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/similarities_avg_nemo.pdf}
    \caption{Llama Nemotron 49B}
    \label{fig:sims_avg_nemo}
\end{subfigure}

\vspace{1em}

\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/similarities_avg_seed_instruct.pdf}
    \caption{Seed-OSS-36B-Instruct}
    \label{fig:sims_avg_seed}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/similarities_avg_seed_base.pdf}
    \caption{Seed-OSS-36B-Base}
    \label{fig:sims_avg_seed_base}
\end{subfigure}

\caption{Similarities with cross-naming average representations across different reasoning models. Each plot shows average similarities of centered action/predicate representations from all timestamps in Mystery BlocksWorld traces with cross-naming representations extracted at 7k tokens for (a) Qwen-32B DeepSeek, (b) Llama Nemotron 49B, (c) Seed-OSS-36B-Instruct, and (d) Seed-OSS-36B-Base \citep{seed2025seed-oss}. All models exhibit progressive convergence toward cross-naming average representations, with similarity increasing substantially during reasoning and plateauing around 7k tokens. Similarities between different actions become increasingly negative as reasoning progresses, demonstrating that representational adaptation is a general property of extended reasoning rather than model-specific behavior. Similarity growth for Seed-Base is slightly slower than for Seed-Instruct.}
\label{fig:sims_avg_all_models}
\end{figure}
\section{Hyperparameters and Experimental Configuration}
\label{app:hyperparameters}

This section provides a comprehensive overview of all hyperparameters and experimental configurations used throughout our analysis.

\subsection{Representation Collection}
\label{app:hyperparams_representation}

\begin{itemize}
    \item \textbf{Token window size} ($w$): Used to identify action/predicate tokens around each timestamp. 200 is selected to contain at least 10 action mentions across the selected batch size.
    \item \textbf{Extraction timestamps}: 2k, 4k, 7k, 10k tokens
    \item \textbf{Batch size} ($b$): Number of reasoning traces per batch for representation extraction. 40 selected, since average mystery accuracy for QwQ was 0.2, giving 40 correctly solved puzzles from a 200-puzzle dataset. 
    \item \textbf{Layers analyzed}: All layers from 0 to model depth
    \item \textbf{Number of puzzles for representation collection}: 40 correctly solved puzzles for steering vectors, full dataset for general analysis
    \item \textbf{Layer selection for representation analysis}: We selected layer 40 as a layer on which the separation of action representation converged and does not change much later. We also preformed analysis on layer 30, which did not lead to any significantly different results, so we did not include it.
\end{itemize}

\subsection{Positive Steering}
\label{app:hyperparams_positive}

\begin{itemize}
    \item \textbf{Steering scale} ($s$): $\frac{2}{3}$ (selected after sweep, see \Cref{fig:20_sweep})
    \item \textbf{Steering window}: $[t_{\text{start}}, t_{\text{end}}) = [1500, 2500]$ tokens
    \item \textbf{Layers tested}: 1, 5, 10, 20, 30, 40, 50, 60
    \item \textbf{Intervention dataset}: 100 held-out 4-block problem rollouts
    \item \textbf{Steering vector extraction timestamp}: 7k tokens (for layer-wise analysis)
\end{itemize}

\subsection{Symbolic Patching}
\label{app:hyperparams_patching}

\begin{itemize}
    \item \textbf{Patching window}: $[2000, 4000]$ tokens
    \item \textbf{End layers tested}: Multiple layers up to selected end layer
    \item \textbf{Scaling factors} ($s$): $\{10, 20\}$
    \item \textbf{Symbolic representation construction}: $\mathbf{r}_{\text{symbolic}}[a] = \mathbf{r}_{\text{mean}} + s \cdot \mathbf{r}_{a}$
    \item \textbf{Control condition}: Shuffled symbolic representations (random permutation)
\end{itemize}

\subsection{Negative Steering}
\label{app:hyperparams_negative}

\begin{itemize}
    \item \textbf{Intervention window}: $[2000, 4000]$ tokens
    \item \textbf{Representation extraction timestamp}: 4k tokens
    \item \textbf{Start layer}: 10
    \item \textbf{End layers tested}: 20, 30
    \item \textbf{Control condition}: Shuffled centered naming representations
\end{itemize}

\subsection{Model Inference}
\label{app:hyperparams_inference}

\begin{itemize}
    \item \textbf{Decoding strategy}: Greedy decoding
    \item \textbf{Maximum sequence length}: 24,576 tokens
    \item \textbf{Temperature}: 0 (greedy)
    \item \textbf{Implementation}: vLLM v0.7.3 with PyTorch forward hooks
\end{itemize}

\subsection{Dataset Configuration}
\label{app:hyperparams_dataset}

\begin{itemize}
    \item \textbf{Number of puzzles}: 300 four-block BlocksWorld puzzles
    \item \textbf{Number of mystery namings}: 15 (primary experiments), 20 (including additional variants)
    \item \textbf{Naming 3 exclusion}: Excluded from representational analyses (recognized as BlocksWorld)
    \item \textbf{Train/test split}: 40 correctly solved puzzles for steering vector extraction, 100 held-out puzzles for steering evaluation
\end{itemize}

\section{Negative steering}
\label{app:negative_steering}

To further validate the Structural Understanding Hypothesis, we conduct an ablation experiment testing whether disrupting representational adaptations decreases accuracy. Since steering interventions can easily degrade performance through general disruption rather than targeted ablation, we use a comparative approach.

We perform interventions across token window [2000, 4000] on multiple layers, subtracting centered naming representations extracted from the 4k timestamp. We use shuffled representations as control, as random vectors provided insufficient baseline strength. We start steering on layer 10 and perform two runs: 1) End layer 20 gives 2.3\%$\pm0.99$\% difference with random.
2) End layer 30 gives 2.9\%$\pm$1.06\%.

\section{Mystery BlocksWorld Naming Variants} \label{app:variants}

% Option 1: Use smaller font and reduce column separation
\begin{table}[H]
\centering
\caption{Action Mappings Across Mystery Namings}
\small % Makes font smaller
\setlength{\tabcolsep}{4pt} % Reduces column separation (default is 6pt)
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Naming} & \textbf{pick up} & \textbf{put down} & \textbf{stack} & \textbf{unstack} \\
\hline
Mystery 1 & attack & succumb & overcome & feast \\
Mystery 2 & illuminate & silence & distill & divest \\
Mystery 3 & tltezi & jchntg & deesdu & xavirm \\
Mystery 4 & swim & fire & deduct & respond \\
Mystery 5 & whisper & calculate & orbit & navigate \\
Mystery 6 & decode & hibernate & thunder & quench \\
Mystery 7 & explore & ripen & weave & bloom \\
Mystery 8 & harvest & ignite & carve & suspend \\
Mystery 9 & construct & demolish & reinforce & collapse \\
Mystery 10 & plant & harvest & nurture & prune \\
Mystery 11 & prosecute & acquit & testify & appeal \\
Mystery 12 & broadcast & receive & encrypt & decode \\
Mystery 13 & whisper & banish & entangle & unmask \\
Mystery 14 & question & resolve & interweave & liberate \\
Mystery 15 & summon & dismiss & fold & unravel \\
\hline
\hline
\multicolumn{5}{|c|}{\textbf{Additional Naming Variants}} \\
\hline
\hline
Mystery 16 & open & close & connect & disconnect \\
Mystery 17 & chop & serve & season & taste \\
Mystery 18 & release & grasp & separate & combine \\
Mystery 19 & transcend & sublimate & actualize & deconstruct \\
Mystery 20 & flixate & grample & chonder & sprill \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Predicate Mappings Across Mystery Namings}
\footnotesize % Even smaller font
\setlength{\tabcolsep}{3pt} % Even tighter column separation
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textbf{Naming} & \textbf{ontable} & \textbf{clear} & \textbf{handempty} & \textbf{holding} & \textbf{on} \\
\hline
Mystery 1 & planet & province & harmony & craves & pain \\
Mystery 2 & aura & essence & nexus & harmonizes & pulse \\
Mystery 3 & oxtslo & adohre & jqlyol & gszswg & ivbmyg \\
Mystery 4 & fever & marble & craving & mines & shadow \\
Mystery 5 & crystal & fountain & autumn & illuminates & legend \\
Mystery 6 & prism & hollow & zenith & echoes & emblem \\
Mystery 7 & fossil & dialect & equinox & fractures & symphony \\
Mystery 8 & nebula & labyrinth & mirage & captivates & cascade \\
Mystery 9 & eclipse & vintage & paradox & resonates & twilight \\
Mystery 10 & crystal & puzzle & vortex & whispers & cipher \\
Mystery 11 & nebula & molecule & anthem & silhouettes & voltage \\
Mystery 12 & horizon & compass & solstice & orbits & quantum \\
Mystery 13 & tethered & unburdened & hollow & shrouds & consuming \\
Mystery 14 & echoing & sovereign & potential & obscures & contemplating \\
Mystery 15 & suspended & timeless & interval & transcends & enveloping \\
\hline
\hline
\multicolumn{6}{|c|}{\textbf{Additional Naming Variants}} \\
\hline
\hline
Mystery 16 & paired & single & balanced & matches & mirrors \\
Mystery 17 & plated & fresh & kitchen & simmering & marinated \\
Mystery 18 & floating & occupied & crowded & repels & avoids \\
Mystery 19 & phenomenal & unmediated & dialectical & instantiates & necessitates \\
Mystery 20 & morkled & thristy & plimmish & vexates & quorbles \\
\hline
\end{tabular}
\end{table}

\section{Mystery Performance Analysis}
\label{app:mystery_performance}

\begin{table}[H]
\centering
\caption{Performance across Mystery BlocksWorld naming variants with steering improvements. Columns with accuracy improvements display the maximum increase on layers 20, 30, 40 and 50.}
\label{tab:mystery_performance_full}
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.95}
\begin{tabular}{|l|c|c|c|p{5cm}|}
\hline
\textbf{Naming} & \textbf{Base} & \textbf{In-Naming} & \textbf{Cross-Naming} & \textbf{Semantic Description} \\
\textbf{Variant} & \textbf{Acc.} & \textbf{Steering} & \textbf{Steering} & \\
\hline
Mystery 1 & 0.33 & +0.10 & +0.11 & Mixed violent/consumption metaphors \\
Mystery 2 & 0.47 & +0.05 & +0.05 & Abstract mystical/spiritual terms \\
Mystery 3 & 0.65 & --- & --- & Random strings \\
Mystery 4 & 0.25 & +0.03 & +0.03 & Mixed physical actions \\
Mystery 5 & 0.24 & -0.01 & +0.01 & Communication/navigation metaphors \\
Mystery 6 & 0.26 & +0.05 & +0.07 & Technical/elemental operations \\
Mystery 7 & 0.19 & +0.02 & +0.03 & Nature/growth cycle \\
Mystery 8 & 0.11 & +0.02 & +0.04 & Agriculture/crafting metaphors \\
Mystery 9 & 0.25 & +0.02 & +0.01 & Construction/destruction cycle \\
Mystery 10 & 0.05 & +0.09 & +0.05 & Coherent gardening domain \\
Mystery 11 & 0.14 & +0.00 & +0.02 & Legal proceedings domain \\
Mystery 12 & 0.16 & +0.06 & +0.02 & Communication technology \\
Mystery 13 & 0.48 & +0.06 & +0.06 & Dark mystical operations \\
Mystery 14 & 0.24 & +0.02 & +0.02 & Abstract philosophical inquiry \\
Mystery 15 & 0.34 & +0.04 & +0.04 & Mystical summoning/manipulation \\
\hline
\hline
\multicolumn{5}{|c|}{\textbf{Additional Variants}} \\
\hline
\hline
Mystery 16 & 0.05 & --- & --- & Reversible operations (open/close) \\
Mystery 17 & 0.27 & --- & --- & Coherent cooking domain \\
Mystery 18 & 0.07 & --- & --- & Physical manipulation verbs \\
Mystery 19 & 0.33 & --- & --- & Abstract philosophical concepts \\
Mystery 20 & 0.29 & --- & --- & Complete nonsense words \\
\hline
\end{tabular}
\end{table}

\Cref{tab:mystery_performance_full} reveals several patterns supporting our hypothesis that semantic coherence impedes abstraction. Namings with coherent alternative domains (Mystery 10: gardening, Mystery 11: legal proceedings, Mystery 16: reversible operations) achieve the lowest base accuracies, while abstract or semantically incoherent combinations (Mystery 2, Mystery 13) enable superior performance.

The steering improvement data shows notable heterogeneity across namings. The maximum improvements reported here represent the best performance across layers 20, 30, 40, and 50, as different namings exhibit optimal responsiveness at different depths. Some namings (Mystery 5, Mystery 11) show minimal or no improvement from naming-mean steering, while others (Mystery 1, Mystery 10) demonstrate substantial gains. This suggests that certain semantic structures are more amenable to representational refinement than others. 

\section{Implementation details}

\subsection{Steering engine}
\label{app:engine}
We implement steering using PyTorch forward hooks on top of vLLM v0.7.3 v0 \cite{kwon2023efficient}, which provides substantial performance improvements, reducing experiment runtimes from several hours to tens of minutes compared to alternatives like TransformerLens \citep{nanda2022transformerlens} or NNSight \cite{fiottokaufman2024nnsightndifdemocratizingaccess}. However, this approach has tradeoffs: we must be mindful of cache recomputations, since they recompute representations without steering interventions, and vLLM's optimizations introduce some numerical instability during extended reasoning traces. To address this instability, we run experiments across multiple naming variants. All experiments use greedy decoding with maximum sequence length of 24,576 tokens.

\subsection{Hyperparamters}
\label{app:hyperparams}

We perform positive steering as described in \Cref{sec:steering_positive} on layer 20 using \textbf{naming-mean} representations to determine the optimal steering scale. \Cref{fig:20_sweep} shows that scales $\frac{2}{3}$ and $\frac{4}{5}$ have similar effects. While improvement from $\frac{4}{5}$ is slightly higher, we chose $\frac{2}{3}$ since it has a more stable effect on all namings.

\begin{figure}[h!] \centering 
\includegraphics[width=0.5\textwidth]{figures/layer_20_sweep.pdf}
    \caption{Positive steering results for layer 20 using different scale parameters $s$.}
\label{fig:20_sweep} \end{figure}

\section{Shuffled In-Naming Steering (Exploratory)}
\label{app:shuffled_steering}

\paragraph{Setup.}
In addition to positive steering (\Cref{sec:steering_positive}), we tested a \emph{shuffled in-naming} control. For each naming, we applied a single consistent permutation of centered in-naming vectors across actions/predicates (e.g., \texttt{pick up}$\to$\texttt{stack}, \texttt{stack}$\to$\texttt{put down}, etc.). This preserves per-naming distributional statistics while breaking the action–representation alignment. Interventions used the same window $[1500,2500]$, scale $s=\tfrac{2}{3}$, and norm-preserving update rule as before, run on a reduced subset of layers and namings.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/steering_improvement_layers_shuffled.pdf}
    \caption{\textbf{Accuracy change under shuffled in-naming steering.} See \Cref{app:shuffled_steering}.}
    \label{fig:shuffled_layers}
\end{figure}

\paragraph{Findings.}
\Cref{fig:shuffled_layers} shows a three-phase trend: (i) early layers ($\!\le\!5$) improve relative to baseline, consistent with disruption of surface semantics; (ii) middle layers ($\sim5-20$) degrade performance, likely breaking emerging abstractions; (iii) later layers ($\ge$30) again show improvements, comparable to unshuffled \emph{in-naming}.

\paragraph{Interpretation.}
Early/mid results align with our story: disruption helps initially, but permutations harm once action-specific representations form. Late-layer gains suggest action vectors contain shared structural components, so even mismatched but in-manifold vectors can occasionally assist. 

\paragraph{Caveat.}
These runs covered fewer namings/layers and late-layer effects were heterogeneous. We report them here for transparency; a fuller sweep is left to future work.

\section{Statistical Analysis of Steering Effects}
\label{app:statistical_analysis}

We conducted statistical tests to validate that steering with refined representations improves accuracy over baseline. Our analysis uses one-sample t-tests treating each mystery naming as an independent observation.

\subsection{Test Methodology}

\paragraph{Data Structure.} Our experiments evaluate steering across 14 mystery namings (excluding naming 3) with approximately 100 puzzles per naming (indices 200-300). Each mystery naming provides accuracy measurements under multiple conditions: baseline (no steering), in-naming steering, cross-naming steering, and random Gaussian steering at various layers.

\paragraph{Statistical Test.} We employ one-sample t-tests to assess whether mean accuracy improvements across mystery namings significantly exceed zero. Each mystery naming serves as an independent observation, with the null hypothesis $H_0: \mu_{\text{improvement}} = 0$. We use one-tailed tests since we hypothesize positive improvements. The test statistic is:
\begin{equation}
    t = \frac{\bar{\Delta}}{\text{SE}(\Delta)} = \frac{\bar{\Delta}}{s_\Delta / \sqrt{n}}
\end{equation}
where $\bar{\Delta}$ is the mean improvement across $n=14$ mystery namings, $s_\Delta$ is the sample standard deviation, and $\text{SE}(\Delta)$ is the standard error.

\subsection{Results}

Table~\ref{tab:steering_improvements} shows that several steering conditions produce statistically significant improvements over baseline. Layer 20 in-naming ($p = 0.042$), layer 20 cross-naming ($p = 0.044$), and layer 40 cross-naming ($p = 0.021$) all achieve significance at $\alpha = 0.05$, with mean improvements ranging from 1.4\% to 1.8\%.

\begin{table}[h!]
\centering
\caption{Accuracy improvements from steering interventions. Mean improvements and standard errors (SE) are computed across 14 mystery namings. Significance levels: * $p < 0.05$, ** $p < 0.01$, *** $p < 0.001$, ns = not significant.}
\label{tab:steering_improvements}
\small
\begin{tabular}{lccccl}
\toprule
\textbf{Condition} & \textbf{Mean $\Delta$} & \textbf{SE} & \textbf{t-statistic} & \textbf{p-value} & \textbf{Sig.} \\
\midrule
Layer 20 In-naming & 1.57\% & 0.84\% & 1.878 & 0.042 & * \\
Layer 20 Cross-naming & 1.79\% & 0.97\% & 1.846 & 0.044 & * \\
Layer 20 Random Gaussian & $-0.36\%$ & 1.08\% & $-0.332$ & 0.627 & ns \\
\midrule
Layer 30 In-naming & 0.64\% & 1.29\% & 0.500 & 0.313 & ns \\
Layer 30 Cross-naming & 1.36\% & 1.20\% & 1.133 & 0.139 & ns \\
\midrule
Layer 40 In-naming & 0.57\% & 0.92\% & 0.618 & 0.274 & ns \\
Layer 40 Cross-naming & 1.43\% & 0.64\% & 2.249 & 0.021 & * \\
\midrule
Layer 50 In-naming & 0.36\% & 1.01\% & 0.352 & 0.365 & ns \\
Layer 50 Cross-naming & 0.93\% & 0.61\% & 1.531 & 0.075 & ns \\
\bottomrule
\end{tabular}
\end{table}

Random Gaussian steering at layer 20 shows negative mean improvement ($-0.36\%$, $p = 0.627$), confirming that improvements from structured representations are not artifacts of random perturbations. Cross-naming representations show numerically higher improvements than in-naming representations at all tested layers, with the strongest and most consistent effects observed at layer 40 where cross-naming achieves 1.43\% improvement ($p = 0.021$) compared to 0.57\% for in-naming ($p = 0.274$).

\subsection{Discussion}

Our statistical analysis provides evidence that steering with refined representations improves accuracy over baseline, with the strongest effects observed at layers 20 and 40 for cross-naming steering. The significant improvements at multiple layers, combined with the lack of improvement from random Gaussian controls, support our hypothesis that representational adaptations during reasoning contain meaningful structural information that causally contributes to problem-solving performance.

The moderate effect sizes (1.4-1.8\% for significant conditions) and variability across mystery namings reflect the challenging nature of using steering with long reasoning rollouts.

\section{Use of Large Language Models (LLMs)}

In accordance with ICLR 2026 policy requirements, we disclose the following use of large language models in the preparation of this paper:

\begin{enumerate}
    \item \textbf{Writing assistance}: LLMs were used to improve clarity, grammar, and overall readability of the manuscript text.
    
    \item \textbf{Terminology generation and classification}: Claude (Anthropic) was used to generate alternative Mystery namings and assess semantic relationships between generated terms.

    \item \textbf{Code generation}: LLMs were used to generate plotting code and other boilerplate code for data visualization and routine programming tasks.
\end{enumerate}

\end{document}
